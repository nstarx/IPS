<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>NeurIPS 2025 ‚Äì Best Paper Awards Dashboard</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root {
      --bg: #050816;
      --bg-elevated: #070c1f;
      --card: #0b1229;
      --accent: #4f46e5;
      --accent-soft: rgba(79, 70, 229, 0.12);
      --accent-2: #06b6d4;
      --text-main: #f9fafb;
      --text-muted: #9ca3af;
      --badge-bg: rgba(148, 163, 184, 0.12);
      --border-subtle: rgba(148, 163, 184, 0.25);
      --radius-xl: 18px;
      --radius-2xl: 24px;
      --shadow-soft: 0 20px 40px rgba(15, 23, 42, 0.65);
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "SF Pro Text",
        "Segoe UI", sans-serif;
      background: radial-gradient(circle at top, #0f172a 0, #020617 50%, #000 100%);
      color: var(--text-main);
      line-height: 1.5;
      padding: 24px;
    }

    .dashboard {
      max-width: 1200px;
      margin: 0 auto 80px auto;
    }

    /* HEADER */

    .hero {
      background: radial-gradient(circle at top left, #1d2a4b 0, #020617 55%);
      border-radius: 32px;
      padding: 28px 24px 22px 24px;
      border: 1px solid rgba(148, 163, 184, 0.2);
      box-shadow: var(--shadow-soft);
      position: relative;
      overflow: hidden;
    }

    .hero::before {
      content: "";
      position: absolute;
      inset: -10%;
      background:
        radial-gradient(circle at 10% 0, rgba(56, 189, 248, 0.22), transparent 55%),
        radial-gradient(circle at 90% 0, rgba(129, 140, 248, 0.24), transparent 55%);
      opacity: 0.8;
      mix-blend-mode: screen;
      pointer-events: none;
    }

    .hero-inner {
      position: relative;
      z-index: 1;
      display: flex;
      flex-direction: column;
      gap: 16px;
    }

    .hero-top {
      display: flex;
      flex-wrap: wrap;
      gap: 16px;
      align-items: center;
      justify-content: space-between;
    }

    .hero-title-block h1 {
      margin: 0;
      font-size: clamp(1.6rem, 2.4vw + 1rem, 2.1rem);
      letter-spacing: 0.02em;
    }

    .hero-title-block p {
      margin: 6px 0 0 0;
      color: var(--text-muted);
      font-size: 0.92rem;
    }

    .hero-pill {
      padding: 6px 12px;
      border-radius: 999px;
      background: rgba(15, 23, 42, 0.85);
      border: 1px solid rgba(148, 163, 184, 0.4);
      display: inline-flex;
      align-items: center;
      gap: 8px;
      font-size: 0.8rem;
      color: #e5e7eb;
      white-space: nowrap;
    }

    .hero-pill span.dot {
      width: 7px;
      height: 7px;
      border-radius: 999px;
      background: #22c55e;
      box-shadow: 0 0 0 6px rgba(34, 197, 94, 0.18);
    }

    .hero-bottom {
      display: flex;
      flex-wrap: wrap;
      gap: 12px;
      margin-top: 6px;
    }

    .hero-tag {
      font-size: 0.78rem;
      padding: 6px 10px;
      border-radius: 999px;
      background: rgba(15, 23, 42, 0.85);
      border: 1px solid rgba(148, 163, 184, 0.35);
      color: var(--text-muted);
    }

    .hero-tag strong {
      color: #e5e7eb;
      font-weight: 600;
    }

    /* STATS */

    .stats-row {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));
      gap: 14px;
      margin-top: 18px;
    }

    .stat-card {
      background: linear-gradient(
        135deg,
        rgba(15, 23, 42, 0.9),
        rgba(15, 23, 42, 0.85)
      );
      border-radius: var(--radius-xl);
      padding: 10px 12px;
      border: 1px solid rgba(148, 163, 184, 0.35);
      display: flex;
      justify-content: space-between;
      align-items: flex-start;
      gap: 8px;
    }

    .stat-main {
      display: flex;
      flex-direction: column;
      gap: 2px;
    }

    .stat-label {
      font-size: 0.78rem;
      text-transform: uppercase;
      letter-spacing: 0.06em;
      color: var(--text-muted);
    }

    .stat-value {
      font-size: 1.4rem;
      font-weight: 650;
    }

    .stat-sub {
      font-size: 0.76rem;
      color: var(--text-muted);
    }

    .stat-icon {
      font-size: 1.4rem;
      opacity: 0.9;
    }

    /* SECTION TITLES */

    .section-title {
      margin: 30px 0 10px 0;
      display: flex;
      justify-content: space-between;
      align-items: baseline;
      gap: 8px;
    }

    .section-title h2 {
      margin: 0;
      font-size: 1.1rem;
      letter-spacing: 0.05em;
      text-transform: uppercase;
      color: #e5e7eb;
    }

    .section-title span {
      font-size: 0.8rem;
      color: var(--text-muted);
    }

    /* THEME LEGEND */

    .theme-legend {
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
      margin-bottom: 10px;
    }

    .theme-pill {
      font-size: 0.76rem;
      padding: 6px 10px;
      border-radius: 999px;
      background: rgba(15, 23, 42, 0.9);
      border: 1px solid rgba(148, 163, 184, 0.3);
      color: var(--text-muted);
      display: inline-flex;
      align-items: center;
      gap: 6px;
    }

    .theme-dot {
      width: 9px;
      height: 9px;
      border-radius: 999px;
      background: var(--accent-soft);
      box-shadow: 0 0 0 1px rgba(148, 163, 184, 0.4);
    }

    .theme-pill[data-theme="llm"] .theme-dot {
      background: #4f46e5;
    }

    .theme-pill[data-theme="rl"] .theme-dot {
      background: #22c55e;
    }

    .theme-pill[data-theme="diffusion"] .theme-dot {
      background: #06b6d4;
    }

    .theme-pill[data-theme="theory"] .theme-dot {
      background: #eab308;
    }

    .theme-pill[data-theme="benchmarks"] .theme-dot {
      background: #ec4899;
    }

    /* GRIDS */

    .cards-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
      gap: 14px;
    }

    /* PAPER CARDS */

    .paper-card {
      background: radial-gradient(circle at top left, #111827, #020617 55%);
      border-radius: var(--radius-2xl);
      padding: 16px 15px 14px 15px;
      border: 1px solid var(--border-subtle);
      box-shadow: var(--shadow-soft);
      position: relative;
      overflow: hidden;
    }

    .paper-card::after {
      content: "";
      position: absolute;
      inset: 0;
      background: radial-gradient(
        circle at top right,
        rgba(79, 70, 229, 0.18),
        transparent 55%
      );
      opacity: 0;
      transition: opacity 0.18s ease-out;
      pointer-events: none;
    }

    .paper-card:hover::after {
      opacity: 1;
    }

    .paper-header {
      display: flex;
      align-items: flex-start;
      justify-content: space-between;
      gap: 10px;
      margin-bottom: 8px;
    }

    .paper-title {
      font-size: 0.94rem;
      font-weight: 600;
      margin: 0;
    }

    .paper-badge {
      font-size: 0.72rem;
      padding: 5px 10px;
      border-radius: 999px;
      background: var(--accent-soft);
      border: 1px solid rgba(129, 140, 248, 0.8);
      color: #c7d2fe;
      text-transform: uppercase;
      letter-spacing: 0.08em;
      white-space: nowrap;
    }

    .paper-badge[data-type="datasets"] {
      border-color: rgba(244, 114, 182, 0.9);
      background: rgba(244, 114, 182, 0.12);
      color: #f9a8d4;
    }

    .paper-badge[data-type="runner"] {
      border-color: rgba(56, 189, 248, 0.8);
      background: rgba(56, 189, 248, 0.12);
      color: #bae6fd;
    }

    .paper-subtitle {
      font-size: 0.8rem;
      color: var(--text-muted);
      margin: 0 0 6px 0;
    }

    .paper-themes {
      display: flex;
      flex-wrap: wrap;
      gap: 6px;
      margin-bottom: 7px;
    }

    .paper-chip {
      font-size: 0.72rem;
      padding: 3px 8px;
      border-radius: 999px;
      background: var(--badge-bg);
      border: 1px solid rgba(148, 163, 184, 0.35);
      color: #e5e7eb;
      display: inline-flex;
      align-items: center;
      gap: 4px;
    }

    .paper-chip span.icon {
      font-size: 0.9em;
      opacity: 0.9;
    }

    .paper-summary {
      font-size: 0.8rem;
      color: var(--text-muted);
      margin: 0 0 8px 0;
    }

    .paper-meta {
      display: flex;
      justify-content: space-between;
      align-items: center;
      gap: 8px;
      font-size: 0.75rem;
      color: var(--text-muted);
      border-top: 1px dashed rgba(148, 163, 184, 0.3);
      padding-top: 7px;
      margin-top: 5px;
    }

    .authors {
      max-width: 75%;
    }

    .authors span {
      white-space: nowrap;
      overflow: hidden;
      text-overflow: ellipsis;
    }

    .pill-link {
      font-size: 0.74rem;
      padding: 4px 9px;
      border-radius: 999px;
      border: 1px solid rgba(148, 163, 184, 0.6);
      text-decoration: none;
      color: #e5e7eb;
      display: inline-flex;
      align-items: center;
      gap: 4px;
      background: rgba(15, 23, 42, 0.9);
    }

    .pill-link span {
      font-size: 0.9em;
    }

    /* FOOTNOTE */

    .footer-note {
      margin-top: 24px;
      font-size: 0.78rem;
      color: var(--text-muted);
      text-align: right;
    }

    @media (max-width: 640px) {
      body {
        padding: 16px;
      }
      .hero {
        padding: 20px 16px 16px 16px;
      }
      .paper-meta {
        flex-direction: column;
        align-items: flex-start;
      }
      .authors {
        max-width: 100%;
      }
    }
  </style>
</head>
<body>
  <main class="dashboard">
    <!-- HERO / HEADER -->
    <section class="hero">
      <div class="hero-inner">
        <div class="hero-top">
          <div class="hero-title-block">
            <h1>NeurIPS 2025 &mdash; Best Paper Awards</h1>
            <p>
              Snapshot of the four Best Papers and three Runners-Up awarded at
              NeurIPS 2025, spanning LLMs, RL, diffusion models, theory and
              benchmarks.
            </p>
          </div>
          <div class="hero-pill">
            <span class="dot"></span>
            <span>Announced &middot; November&nbsp;26,&nbsp;2025</span>
          </div>
        </div>

        <div class="hero-bottom">
          <div class="hero-tag">
            <strong>7 papers</strong>&nbsp;&bull;&nbsp;4 Best&nbsp;&middot;&nbsp;3 Runner-up
          </div>
          <div class="hero-tag">
            <strong>Tracks</strong>&nbsp;&bull;&nbsp;Main + Datasets &amp; Benchmarks
          </div>
          <div class="hero-tag">
            <strong>Domains</strong>&nbsp;&bull;&nbsp;Generative models, RL, LLMs, Theory
          </div>
          <a href="neurips_explorer.html" class="hero-tag" style="text-decoration: none; cursor: pointer;">
            <strong>üîç Explore</strong>&nbsp;&bull;&nbsp;Full NeurIPS 2025 Explorer
          </a>
        </div>

        <div class="stats-row">
          <article class="stat-card">
            <div class="stat-main">
              <span class="stat-label">Total Awards</span>
              <span class="stat-value">7</span>
              <span class="stat-sub">4 Best &middot; 3 Runner-up</span>
            </div>
            <div class="stat-icon">üèÜ</div>
          </article>

          <article class="stat-card">
            <div class="stat-main">
              <span class="stat-label">Key Themes</span>
              <span class="stat-value">5</span>
              <span class="stat-sub">LLMs, RL, diffusion, theory, benchmarks</span>
            </div>
            <div class="stat-icon">üß†</div>
          </article>

          <article class="stat-card">
            <div class="stat-main">
              <span class="stat-label">Awarded Tracks</span>
              <span class="stat-value">2</span>
              <span class="stat-sub">Main + Datasets &amp; Benchmarks</span>
            </div>
            <div class="stat-icon">üìä</div>
          </article>

          <article class="stat-card">
            <div class="stat-main">
              <span class="stat-label">Research Axes</span>
              <span class="stat-value">Method + Theory</span>
              <span class="stat-sub">Architectures, dynamics, limits</span>
            </div>
            <div class="stat-icon">üß¨</div>
          </article>
        </div>
      </div>
    </section>

    <!-- THEME LEGEND -->
    <section>
      <div class="section-title">
        <h2>Theme Legend</h2>
        <span>Color-coded by dominant research area</span>
      </div>
      <div class="theme-legend">
        <div class="theme-pill" data-theme="llm">
          <span class="theme-dot"></span>
          LLMs &amp; Architectures
        </div>
        <div class="theme-pill" data-theme="rl">
          <span class="theme-dot"></span>
          Reinforcement Learning
        </div>
        <div class="theme-pill" data-theme="diffusion">
          <span class="theme-dot"></span>
          Diffusion &amp; Generative Models
        </div>
        <div class="theme-pill" data-theme="theory">
          <span class="theme-dot"></span>
          Learning Theory &amp; Scaling
        </div>
        <div class="theme-pill" data-theme="benchmarks">
          <span class="theme-dot"></span>
          Datasets &amp; Benchmarks
        </div>
      </div>
    </section>

    <!-- BEST PAPERS -->
    <section>
      <div class="section-title">
        <h2>Best Papers</h2>
        <span>Four Best Papers (incl. one in Datasets &amp; Benchmarks)</span>
      </div>

      <div class="cards-grid">
        <!-- Artificial Hivemind -->
        <article class="paper-card">
          <div class="paper-header">
            <h3 class="paper-title">
              Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)
            </h3>
            <div class="paper-badge" data-type="datasets">Best Paper &middot; D&amp;B</div>
          </div>
          <p class="paper-subtitle">
            Infinity-Chat benchmark for open-ended prompts + large-scale analysis of diversity and ‚Äúartificial hivemind‚Äù effects in LLM generations.
          </p>
          <div class="paper-themes">
            <span class="paper-chip">
              <span class="icon">ü§ñ</span> LLM Behaviour
            </span>
            <span class="paper-chip">
              <span class="icon">üìä</span> Datasets &amp; Benchmarks
            </span>
            <span class="paper-chip">
              <span class="icon">üåê</span> Diversity &amp; Society
            </span>
          </div>
          <p class="paper-summary">
            Introduces Infinity-Chat (26k open-ended queries + dense human annotations) and shows strong intra-model repetition and inter-model homogeneity, raising concerns about long-term creativity and value pluralism.
          </p>
          <div class="paper-meta">
            <div class="authors">
              <span>Liwei Jiang et&nbsp;al.</span>
            </div>
            <a class="pill-link" href="https://openreview.net/forum?id=5" target="_blank" rel="noopener">
              <span>View paper</span> <span>‚Üó</span>
            </a>
          </div>
        </article>

        <!-- Gated Attention -->
        <article class="paper-card">
          <div class="paper-header">
            <h3 class="paper-title">
              Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free
            </h3>
            <div class="paper-badge">Best Paper</div>
          </div>
          <p class="paper-subtitle">
            Simple head-specific sigmoid gating after SDPA that stabilizes training, reduces attention sink, and improves long-context performance in large-scale LLMs.
          </p>
          <div class="paper-themes">
            <span class="paper-chip">
              <span class="icon">üß†</span> LLM Architectures
            </span>
            <span class="paper-chip">
              <span class="icon">üìà</span> Scaling &amp; Stability
            </span>
            <span class="paper-chip">
              <span class="icon">üß©</span> Gating Mechanisms
            </span>
          </div>
          <p class="paper-summary">
            Compares dozens of gated-attention variants on 15B MoE and 1.7B dense models and finds a consistently strong win for a single, easy-to-implement gating design now used in Qwen3-Next models.
          </p>
          <div class="paper-meta">
            <div class="authors">
              <span>Zihan Qiu et&nbsp;al.</span>
            </div>
            <a class="pill-link" href="https://openreview.net/forum?id=15" target="_blank" rel="noopener">
              <span>View paper</span> <span>‚Üó</span>
            </a>
          </div>
        </article>

        <!-- 1000 Layer Networks for Self-Supervised RL -->
        <article class="paper-card">
          <div class="paper-header">
            <h3 class="paper-title">
              1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities
            </h3>
            <div class="paper-badge">Best Paper</div>
          </div>
          <p class="paper-subtitle">
            Demonstrates that very deep (up to 1024-layer) self-supervised RL agents can achieve strong goal-reaching performance without explicit rewards.
          </p>
          <div class="paper-themes">
            <span class="paper-chip">
              <span class="icon">üéÆ</span> Reinforcement Learning
            </span>
            <span class="paper-chip">
              <span class="icon">üìè</span> Depth Scaling
            </span>
            <span class="paper-chip">
              <span class="icon">ü¶æ</span> Locomotion &amp; Control
            </span>
          </div>
          <p class="paper-summary">
            Challenges the assumption that RL is incompatible with very deep nets. Using contrastive, goal-conditioned self-supervision, depth scaling yields higher success rates and richer emergent behaviours in simulated tasks.
          </p>
          <div class="paper-meta">
            <div class="authors">
              <span>Kevin Wang et&nbsp;al.</span>
            </div>
            <a class="pill-link" href="https://openreview.net/forum?id=32" target="_blank" rel="noopener">
              <span>View paper</span> <span>‚Üó</span>
            </a>
          </div>
        </article>

        <!-- Why Diffusion Models Don‚Äôt Memorize -->
        <article class="paper-card">
          <div class="paper-header">
            <h3 class="paper-title">
              Why Diffusion Models Don‚Äôt Memorize: The Role of Implicit Dynamical Regularization in Training
            </h3>
            <div class="paper-badge">Best Paper</div>
          </div>
          <p class="paper-subtitle">
            Explains how training dynamics create a window where diffusion models generalize well before memorization sets in, even in over-parameterized regimes.
          </p>
          <div class="paper-themes">
            <span class="paper-chip">
              <span class="icon">üåÄ</span> Diffusion Models
            </span>
            <span class="paper-chip">
              <span class="icon">üìê</span> Implicit Regularization
            </span>
            <span class="paper-chip">
              <span class="icon">‚öñÔ∏è</span> Generalization vs. Memorization
            </span>
          </div>
          <p class="paper-summary">
            Identifies two characteristic timescales for diffusion training and shows that the generalization window grows with dataset size, tying practical success to provable dynamical regularization.
          </p>
          <div class="paper-meta">
            <div class="authors">
              <span>Tony Bonnaire et&nbsp;al.</span>
            </div>
            <a class="pill-link" href="https://openreview.net/forum?id=38" target="_blank" rel="noopener">
              <span>View paper</span> <span>‚Üó</span>
            </a>
          </div>
        </article>
      </div>
    </section>

    <!-- RUNNERS UP -->
    <section>
      <div class="section-title">
        <h2>Runner-Up Papers</h2>
        <span>Three additional papers recognized for outstanding contributions</span>
      </div>

      <div class="cards-grid">
        <!-- RL & Reasoning in LLMs -->
        <article class="paper-card">
          <div class="paper-header">
            <h3 class="paper-title">
              Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
            </h3>
            <div class="paper-badge" data-type="runner">Runner-up</div>
          </div>
          <p class="paper-subtitle">
            Systematic probe of RL with verifiable rewards (RLVR) shows improved sampling efficiency but no fundamentally new reasoning patterns beyond the base model.
          </p>
          <div class="paper-themes">
            <span class="paper-chip">
              <span class="icon">üß†</span> LLM Reasoning
            </span>
            <span class="paper-chip">
              <span class="icon">üéØ</span> RLVR
            </span>
            <span class="paper-chip">
              <span class="icon">üß™</span> Evaluation &amp; Limits
            </span>
          </div>
          <p class="paper-summary">
            Across models, algorithms and benchmarks, RLVR narrows exploration and amplifies rewarded trajectories without expanding the underlying reasoning frontier; distillation is found to add truly new reasoning patterns.
          </p>
          <div class="paper-meta">
            <div class="authors">
              <span>Yang Yue et&nbsp;al.</span>
            </div>
            <a class="pill-link" href="https://openreview.net/forum?id=43" target="_blank" rel="noopener">
              <span>View paper</span> <span>‚Üó</span>
            </a>
          </div>
        </article>

        <!-- Transductive Online Learning -->
        <article class="paper-card">
          <div class="paper-header">
            <h3 class="paper-title">
              Optimal Mistake Bounds for Transductive Online Learning
            </h3>
            <div class="paper-badge" data-type="runner">Runner-up</div>
          </div>
          <p class="paper-subtitle">
            Resolves a 30-year-old open problem on the value of unlabeled data in online learning with tight Œ©(‚àöd) / O(‚àöd) bounds.
          </p>
          <div class="paper-themes">
            <span class="paper-chip">
              <span class="icon">üìö</span> Learning Theory
            </span>
            <span class="paper-chip">
              <span class="icon">üìâ</span> Mistake Bounds
            </span>
            <span class="paper-chip">
              <span class="icon">üîç</span> Transductive Setting
            </span>
          </div>
          <p class="paper-summary">
            Shows a quadratic gap between transductive and standard online learning and clarifies when advanced access to unlabeled instances yields exponential improvements over prior lower bounds.
          </p>
          <div class="paper-meta">
            <div class="authors">
              <span>Zachary Chase et&nbsp;al.</span>
            </div>
            <a class="pill-link" href="https://openreview.net/forum?id=52" target="_blank" rel="noopener">
              <span>View paper</span> <span>‚Üó</span>
            </a>
          </div>
        </article>

        <!-- Superposition & Scaling -->
        <article class="paper-card">
          <div class="paper-header">
            <h3 class="paper-title">
              Superposition Yields Robust Neural Scaling
            </h3>
            <div class="paper-badge" data-type="runner">Runner-up</div>
          </div>
          <p class="paper-subtitle">
            Argues that representation superposition is a key mechanism behind neural scaling laws in large models.
          </p>
          <div class="paper-themes">
            <span class="paper-chip">
              <span class="icon">üß¨</span> Neural Scaling Laws
            </span>
            <span class="paper-chip">
              <span class="icon">üß©</span> Feature Superposition
            </span>
            <span class="paper-chip">
              <span class="icon">üìè</span> Chinchilla Regime
            </span>
          </div>
          <p class="paper-summary">
            Uses a controlled toy model plus empirical analysis of open LLMs to show that strong superposition naturally produces inverse-dimension scaling of loss, explaining when scaling laws hold or break.
          </p>
          <div class="paper-meta">
            <div class="authors">
              <span>Yizhou Liu et&nbsp;al.</span>
            </div>
            <a class="pill-link" href="https://openreview.net/forum?id=57" target="_blank" rel="noopener">
              <span>View paper</span> <span>‚Üó</span>
            </a>
          </div>
        </article>
      </div>
    </section>

    <p class="footer-note">
      Data source: NeurIPS Blog &mdash; ‚ÄúAnnouncing the NeurIPS 2025 Best Paper Awards‚Äù
      (Nov&nbsp;26,&nbsp;2025).
    </p>
  </main>
</body>
</html>


